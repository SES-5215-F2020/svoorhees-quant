---
title: "Assignment 2: Quantitive Analysis"
Date: 9/16/2020
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
    theme: cosmo
---

    
## **Overview of Assignment**

The intention of this assignment is to familiarize with calculating distributions for continuous variables and proportions for discrete variables. For this exercise, I will be pulling in my CSV file created for the first assignment in this course. 

This dataset(assignment_1_vars) includes the following 6 variables I will be calculating distributions & proportions for:


> **credits**: I'm using the incredibly helpful floating ToC pulled from your tutorial code in GitHub. 

#### **Variables** 

**Continuous** <br>
1. Household Income (HINCP)  <br>
  +  *In this data I will be using the first way of complling data identified in tutorials* <br>
2. Number of People in Household (NP) <br>
  +  *In the remaining variables, I will be using the second way of complling data identified in tutorials* <br>
3.Commute to work (JWMNP) <br>
4. Age of Person (AGEP) <br>

**Categorical** <br> 
5. Decade House Constructed <br>
6. Racial Demographics <br>


##### Downloading Libraries and Showing Variables:
```{r, message = FALSE, results='hide'}
 library(tidycensus)
library(tidyverse)
library(ggplot2)
library(ggthemes)
```

#####  Load Household Data Set & Housekeeping
```{r, message = FALSE, results='hide'}
# Load CSV
household_data <- read_csv("assignment1_vars.csv")
#Remove Scientific Notation
options(scipen = 999)
```

--------------------

## **Continuous Distributions**

### 1. Distribution for Household Income

> **For Household Income In New Mexico:** <br>
Mean: $79,843.00 <br>
Standard Deviation: $64,093.08 <br>
Interquartile Range: $30,500 to 100,000 $<br>
95 % Confidence Interval: $57,667.38 to $67,072.59 <br>


>**Discussion of Income**
I Realized that I wanted to filter my data set to remove negative values for income. I came to this conclusion that I should filter for non-negative values when talking with Emma Colley. I did this in the code from Assignment 1 so it is not shown in this R Markdown document. 

It is really striking to me how New Mexico compares to other states. The Interquartile range is the measurement that is most revealing to me. Overall the histogram is skewed right with a high concentration around 75,000$. The skew is enough that it might make sense to put this data into income quartiles. 

**Historgram**
```{r}
hist(household_data$HINCP) 
```



**Summary: Mean, Interquartile Range**
```{r}
#Summary
HINCP_summary <- summary(household_data$HINCP)
HINCP_summary
```

**Standard Deviation**
```{r}
#Standard Deviation
sd(household_data$HINCP)
```

**95% Confidence Interval** 
```{r}
#Step One: Take a One Sample T-Test
conf_int<- t.test(household_data$HINCP)
conf_int
# Take the mean (which will be the same as the sample mean)
conf_int$estimate

#Calculate the LOWER limit
conf_int$conf.int[1]

#Calculate the UPPER limit
conf_int$conf.int[2]
```


--------------------



### 2. Distribution for Number of People In Household
> **For Number of People in Household In New Mexico:** <br>
Mean: 3.07 People <br>
Standard Deviation: 1.71  <br>
Interquartile Range:2 people - 4 people $<br>
95 % Confidence Interval: 3.05 -3.11 <br>

> **Discussion** This pretty much checks out with what I would have guessed. I don't know if I would consider this a normal distribution since there is such a sharp peak at 2 members in a househod. 

```{r}
ggplot(household_data, aes(x=NP)) +
  geom_histogram() +
  scale_x_continuous(breaks=seq(0,13, by =1)) +
  theme_wsj()
```


```{r}
mean(household_data$NP)
median(household_data$NP)
quantile(household_data$NP)
sd(household_data$NP)
```

**95% Confidence Interval** 
```{r}
#Step One: Take a One Sample T-Test
conf_int<- t.test(household_data$NP)
conf_int
# Take the mean (which will be the same as the sample mean)
conf_int$estimate

#Calculate the LOWER limit
conf_int$conf.int[1]

#Calculate the UPPER limit
conf_int$conf.int[2]
```


--------------------



###  3. Distribution for Commute
>**How long is the commute In New Mexico (In Minutes):** <br>
Mean: 9.3 Minutes <br>
Standard Deviation: 17.2 minutes  <br>
Interquartile Range: 0 to 15 minutes $<br>
95 % Confidence Interval: 9.05-9.60 <br>

>**Discussion** Im very curious about the accuracy of this dataset. I think in the future, I might be better off to convert it to a categorical variable. There seems to be a lot of rounding/not reporting. New Mexico doesn't have a "rush hour" and it is uncommon for long commutes, so I could see this being somewhat accurate. But it is hard to believe that there are so many people with no commute. It is a normal-ish distribution if you exclude the less than 5 minute commute times. 

```{r}
ggplot(household_data, aes(x=JWMNP)) +
  geom_histogram() +
  scale_x_continuous(breaks=seq(0,160, by =10)) +
  theme_wsj()
```


```{r}
mean(household_data$JWMNP)
quantile(household_data$JWMNP)
sd(household_data$JWMNP)
```

**95% Confidence Interval** 
```{r}
#Step One: Take a One Sample T-Test
conf_int<- t.test(household_data$JWMNP)
conf_int
# Take the mean (which will be the same as the sample mean)
conf_int$estimate

#Calculate the LOWER limit
conf_int$conf.int[1]

#Calculate the UPPER limit
conf_int$conf.int[2]
```



--------------


### 4. Distribution for Age
>**What is the Age Distribution in New Mexico:** <br>
Mean: 43.3 years old <br>
Standard Deviation: 23.8 years  <br>
Interquartile Range: 23 years old to 63 years old minutes $<br>
95 % Confidence Interval: 42.9 years old  to 43.7 years old  <br>

> **Discussion** People leaving New Mexico in their twenties and coming back when they are older is SUCH a trope of NM, interesting to see that reflected in the data. Definitely not a normal distribution, looks like a bimodal distribution. 

```{r}
ggplot(household_data, aes(x=AGEP)) +
  geom_histogram() +
  scale_x_continuous(name="Age", 
                     breaks = seq(0,100, by = 5),
                     labels = paste(seq(0,100, by = 5),
                     "", sep = "")) +
  theme_wsj()
```


```{r}
mean(household_data$AGEP)
quantile(household_data$AGEP)
sd(household_data$AGEP)
```

**95% Confidence Interval** 
```{r}
#Step One: Take a One Sample T-Test
conf_int<- t.test(household_data$AGEP)
conf_int
# Take the mean (which will be the same as the sample mean)
conf_int$estimate

#Calculate the LOWER limit
conf_int$conf.int[1]

#Calculate the UPPER limit
conf_int$conf.int[2]
```




## **Proportions of Categorical Variables**

### 5. Proportions of Construction Decade
> **Discussion** This seems to really highlight the new construction drop-off in 2008. In the Vis exercise, I feel like I was trying to get fancy with violin plots, when this is what would have helped make the case the clearest. I think is the closest to a normal distribution that I have. 

```{r}
ggplot(household_data, aes(x=development)) +
  geom_bar() +
  theme_wsj()
```
**Proportions & 95% COnfidence Intervals**

```{r}
#Get list of all possible values
decade <- unique(household_data$development)
decade

#Get share of Sample in EACH Category
fourties <- t.test(household_data$development==decade[1])
fifties <- t.test(household_data$development==decade[2])
sixties <- t.test(household_data$development==decade[3])
seventies <- t.test(household_data$development==decade[4])
eighties <- t.test(household_data$development==decade[5])
nineties <- t.test(household_data$development==decade[6])
two_thousands <- t.test(household_data$development==decade[7])
two_thousand_tens <-  t.test(household_data$development==decade[8])

#Get the Shares
shares <- tibble(Decade = c("1940s",
                            "1950s",
                            "1960s",
                            "1970s",
                            "1980s",
                            "1990s",
                            "2000s"),
                 
                 `Share` = c(fourties$estimate,
                             fifties$estimate,
                             sixties$estimate,
                             seventies$estimate,
                             eighties$estimate,
                             nineties$estimate,
                             two_thousands$estimate),
                 
                 Low = c(fourties$conf.int[1],
                             fifties$conf.int[1],
                             sixties$conf.int[1],
                             seventies$conf.int[1],
                             eighties$conf.int[1],
                             nineties$conf.int[1],
                             two_thousands$conf.int[1]),
                  
                 High = c(fourties$conf.int[2],
                             fifties$conf.int[2],
                             sixties$conf.int[2],
                             seventies$conf.int[2],
                             eighties$conf.int[2],
                             nineties$conf.int[2],
                             two_thousands$conf.int[2]))
knitr::kable(shares, caption = "Proportions and 95-percent confidence intervals") 
```
**notes** Ryan Johnson helped me find a missing parenthesis in this code. I owe him my sanity. 

**proportions**
```{r}
#See Unique Values
#unique(household_data$development)
#Pull Into table
#table(household_data$development)
#Create Proportions by dividing above table by total number of observations
#table(household_data$development)/sum(table(household_data$development))

#Or calculate directly using 
#mean(household_data$development == "1970s")
```
**95% COnfidence Interval**
```{r}
#Use T-Test
t.test(household_data$development == "1940s")
```

### 6. Proportions of Racial Demographics

> **Discussion** I really want to break out hispanic ethnicity. Currently the distribution is all lumped into largely one homogenous category. 

```{r}
ggplot(household_data, aes(x=RAC1P_label)) +
  geom_bar() +
  theme_fivethirtyeight()
```
**Proportions & 95% COnfidence Intervals**

Notes: Running into this error, "Error in t.test.default(household_data$RAC1P == decade[1]) : not enough 'x' observations"
cannot seem to get around it. 


```{r}
#Get list of all possible values
races <- unique(household_data$RAC1P_label)
races

#TroubleShooting
table(household_data$RAC1P_label)

#Get share of Sample in EACH Category
white <- t.test(household_data$RAC1P_label==races[1])
american_indian <- t.test(household_data$RAC1P_label==races[2])
two_or_more <- t.test(household_data$RAC1P_label==races[3])
american_indian_tribe_spec <- t.test(household_data$RAC1P_label==races[4])
asian <- t.test(household_data$RAC1P_label==races[5])
black <- t.test(household_data$RAC1P_label==races[6])
some_other_race <- t.test(household_data$RAC1P_label==races[7])
hawaiian_pacific_islander <- t.test(household_data$RAC1P_label==races[8])
alaska_native <- t.test(household_data$RAC1P_label==races[9])

#Get the Shares
shares <- tibble(Decade = c("White",
                            "American Indian",
                            "Two Or More",
                            "American Indian Specific Tribe",
                            "Asian",
                            "Black",
                            "Some Other Race",
                            "Hawaii Pacific Islander",
                            "Alaska Native"),
                 
                 `Share` = c(white$estimate,
                             american_indian$estimate,
                             two_or_more$estimate,
                             american_indian_tribe_spec$estimate,
                             asian$estimate,
                             black$estimate,
                             some_other_race$estimate,
                             hawaiian_pacific_islander$estimate,
                             alaska_native$estimate),
                 
                 Low = c(white$conf.int[1],
                             american_indian$conf.int[1],
                             two_or_more$conf.int[1],
                             american_indian_tribe_spec$conf.int[1],
                             asian$conf.int[1],
                             black$conf.int[1],
                             some_other_race$conf.int[1],
                             hawaiian_pacific_islander$conf.int[1],
                             alaska_native$conf.int[1]),
                  
                 High =c(white$conf.int[2],
                             american_indian$conf.int[2],
                             two_or_more$conf.int[2],
                             american_indian_tribe_spec$conf.int[2],
                             asian$conf.int[2],
                             black$conf.int[2],
                             some_other_race$conf.int[2],
                             hawaiian_pacific_islander$conf.int[2],
                             alaska_native$conf.int[2]))
knitr::kable(shares, caption = "Proportions and 95-percent confidence intervals") 
```





--------------------


## **R Cheat Sheet, References & Troubleshooting**
Hello! I see this area of my assignment as predominantly a resource for myself as I learn R and as a resource to R if I ever need to return after a hiatus (and have forgotten everything I know). 

Feel free to look through, but there is no need to grade anything in this section. 


** Sage's Troubleshooting Checklist** <br/>
0. Breathe. <br/>
1. Is everything spelled/cApiTAlized correctly? <br/>
2. Count your parentheses(), commas, and "quotes" <br/>
    2.1 Check poles %>%, | <br/>
3. Have you run the R chunks that are before the chunk with your error. <br/>
5. What is the last thing you changed? Did you make sure you changed it in ALL the places it needs to be changed?<br/>
6. Breathe. Maybe get a glass of water.  <br/>
7. Type in the ?function() into the Console <br/>
8. Do a Google search for "R Markdown" + Your Problem <br/>
9. Try retyping the function from scratch.
10. Try copying and pasting sections in another R Chunk --> Isolate the Problem <br/>
11. Phone a friend. <br/>

**Common Errors:** <br/>
-If you are having a function not found error, check whether or not you have run your libraries. <br/>
-YAML error, check the very beginning of your code
- 

**If you want to see the values of a variable as they appear in your dataset**
#Race <br/>
#unique(household_data$RAC1P_label) <br/>
#Hispanic <br/>
#unique(household_data$HISP_label) <br/>

### References

**For General R**
R is for Data Science
https://r4ds.had.co.nz/index.html

Data Wrangling Cheat Sheet
https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf

*formatting*
https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf

*For GGPLOT**
Top 50 Graph Types
http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html#Violin%20Plot

GGplot Cheat Sheet
https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf

Color Pallets
https://www.datanovia.com/en/blog/ggplot-colors-best-tricks-you-will-love/#predefined-ggplot-color-palettes


Look into Wes Anderson Themes, WSJ, Economist

**Number Crunching**

Another Way of Pulling Summary <br/>
#mean(household_data$HINCP) <br/>
#quantile(household_data$HINCP) <br/>
#sd(household_data$HINCP) <br/>

Simple Explanation of Dsitributions: 
https://nezumisa.wordpress.com/2013/06/03/different-types-of-distributions/




